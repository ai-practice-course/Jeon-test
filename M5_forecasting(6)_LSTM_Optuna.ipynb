{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "M5 forecasting(6)_LSTM_Optuna.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Z0lvQ1GZOrp",
        "outputId": "b15495b4-55be-4cfe-9e18-abb0c00fd9ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.7/dist-packages (2.10.1)\n",
            "Requirement already satisfied: alembic in /usr/local/lib/python3.7/dist-packages (from optuna) (1.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna) (1.21.6)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.7/dist-packages (from optuna) (6.6.0)\n",
            "Requirement already satisfied: cliff in /usr/local/lib/python3.7/dist-packages (from optuna) (3.10.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from optuna) (3.13)\n",
            "Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (21.3)\n",
            "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.37)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna) (4.64.0)\n",
            "Requirement already satisfied: cmaes>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from optuna) (0.8.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->optuna) (3.0.9)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (4.11.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (1.1.2)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic->optuna) (5.7.1)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.7/dist-packages (from alembic->optuna) (1.2.0)\n",
            "Requirement already satisfied: stevedore>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (3.5.0)\n",
            "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (5.9.0)\n",
            "Requirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (3.3.0)\n",
            "Requirement already satisfied: cmd2>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (2.4.1)\n",
            "Requirement already satisfied: autopage>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (0.5.1)\n",
            "Requirement already satisfied: pyperclip>=1.6 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (1.8.2)\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (4.1.1)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (21.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna) (3.8.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna) (2.0.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "import joblib\n",
        "import pickle\n",
        "import optuna\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "LD8HXYDoZRkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sliding_windows(data, lookback_length, forecast_length):\n",
        "\n",
        "    x = []\n",
        "    y = []\n",
        "    \n",
        "    for i in range(lookback_length, len(data) - forecast_length + 1):\n",
        "        _x = data[(i-lookback_length) : i]\n",
        "        _y = data[i : (i + forecast_length)]\n",
        "        x.append(_x)\n",
        "        y.append(_y)\n",
        "    return np.array(x), np.array(y)\n",
        "\n",
        "\n",
        "def get_data_loader(X, y, batch_size):\n",
        "\n",
        "    x_train, x_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n",
        "\n",
        "    train_ds = TensorDataset(torch.Tensor(x_train), torch.Tensor(y_train))\n",
        "    train_dl = DataLoader(train_ds, batch_size = batch_size)\n",
        "\n",
        "    val_ds = TensorDataset(torch.Tensor(x_val), torch.Tensor(y_val))\n",
        "    val_dl = DataLoader(val_ds, batch_size = batch_size)\n",
        "\n",
        "    input_size = x_train.shape[-1]\n",
        "\n",
        "    return train_dl, val_dl, input_size"
      ],
      "metadata": {
        "id": "hwb_80LTZTJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_train_val = pd.read_csv('sales_train_validation.csv')\n",
        "sales_train_val = sales_train_val.T\n",
        "sales_train_val = sales_train_val.fillna(0)\n",
        "sales_train_val = sales_train_val[6:]\n",
        "scaler = MinMaxScaler(feature_range=(0,1))"
      ],
      "metadata": {
        "id": "ZODzluaFaCxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#timesteps = 14\n",
        "#x_train = []\n",
        "#y_train = []\n",
        "#for i in range(timesteps, 1913):\n",
        "#    x_train.append(sales_train_val[i-timesteps:i])\n",
        "#    y_train.append(sales_train_val[i][0:30490])"
      ],
      "metadata": {
        "id": "ou-YC9kVfubj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#x_train=np.array(x_train, dtype='float16')\n",
        "#y_train=np.array(y_train, dtype='float16')"
      ],
      "metadata": {
        "id": "bVIt9VJQfwYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#x_train.shape,y_train.shape"
      ],
      "metadata": {
        "id": "UvrOz2DSf3_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sales_train_val.reset_index(drop=True, inplace=True)\n",
        "\n",
        "scale_cols = []\n",
        "timesteps = 14\n",
        "for i in range(timesteps, 1913):\n",
        "    scale_cols.append(sales_train_val[i][0:30490])\n",
        "\n",
        "scale_cols = np.array(scale_cols, dtype='float16')\n",
        "\n",
        "# Loockback_period & forecasting_period\n",
        "max_prediction_length = 20\n",
        "lookback_length = 60\n",
        "training_data_max = len(sales_train_val) - max_prediction_length\n",
        "\n",
        "# 학습용 데이터\n",
        "sales_train_val_p = sales_train_val.iloc[:training_data_max, :]\n",
        "training_sales_train_val = scaler.fit_transform(sales_train_val_p)"
      ],
      "metadata": {
        "id": "sIr9vtmDZbFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM은 1 step 뒤의 값만을 예측하므로, forecasting_period를 1로 두고 진행\n",
        "x, y = sliding_windows(training_sales_train_val, lookback_length, 1)"
      ],
      "metadata": {
        "id": "SjpbxwHSbXLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model 정의"
      ],
      "metadata": {
        "id": "lWdA5udDbhhK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.num_layers = num_layers\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        \n",
        "        self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size,\n",
        "                            num_layers = num_layers, batch_first = True)\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_size  * num_layers, num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size, device = x.device))\n",
        "        c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size, device = x.device))\n",
        "        \n",
        "        # Propagate input through LSTM\n",
        "        ula, (h_out, _) = self.lstm(x, (h_0, c_0))\n",
        "        h_out = h_out.view(-1, self.hidden_size * self.num_layers)\n",
        "        out = self.fc(h_out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "R-7gBsKvbinF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(log_interval, model, train_dl, val_dl, optimizer, criterion, epoch):\n",
        "\n",
        "    best_loss = np.inf\n",
        "    for epoch in range(epoch):\n",
        "        train_loss = 0.0\n",
        "        model.train()\n",
        "        for sales_train_val, target in train_dl:\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                sales_train_val, target = sales_train_val.cuda(), target.cuda()\n",
        "                model = model.cuda()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(sales_train_val)\n",
        "            loss = criterion(output, target) # mean-squared error for regression\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        # validation\n",
        "        valid_loss = 0.0\n",
        "        model.eval()\n",
        "        for sales_train_val, target in val_dl:\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                sales_train_val, target = sales_train_val.cuda(), target.cuda()\n",
        "\n",
        "            output = model(sales_train_val)         \n",
        "            loss = criterion(output, target)\n",
        "            valid_loss += loss.item()\n",
        "\n",
        "        if ( epoch % log_interval == 0 ):\n",
        "            print(f'\\n Epoch {epoch} \\t Training Loss: {train_loss / len(train_dl)} \\t Validation Loss: {valid_loss / len(val_dl)} \\n')\n",
        "\n",
        "        if best_loss > (valid_loss / len(val_dl)):\n",
        "            print(f'Validation Loss Decreased({best_loss:.6f}--->{(valid_loss / len(val_dl)):.6f}) \\t Saving The Model')\n",
        "            best_loss = (valid_loss / len(val_dl))\n",
        "            torch.save(model.state_dict(), 'lstm_saved_model.pth')\n",
        "\n",
        "    return best_loss\n",
        "\n",
        "\n",
        "def smape(a, f):\n",
        "    return 1/len(a) * np.sum(2 * np.abs(f-a) / (np.abs(a) + np.abs(f))*100)"
      ],
      "metadata": {
        "id": "2RrwPW2qbmte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aa = x_for_metric\n",
        "tmp = np.append( np.expand_dims(aa[1:, :], 0), np.expand_dims(y_for_metric[2, :], (0,2)), axis=1)\n",
        "tmp.shape"
      ],
      "metadata": {
        "id": "pEdxB_qhcc3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial):\n",
        "\n",
        "    cfg = { \n",
        "            'batch_size' : trial.suggest_categorical('batch_size',[64, 128, 256, 512]), # [64, 128, 256]\n",
        "            'learning_rate' : trial.suggest_loguniform('learning_rate', 1e-3, 1e-1), #trial.suggest_loguniform('learning_rate', 1e-2, 1e-1), # learning rate을 0.01-0.1까지 로그 uniform 분포로 사용\n",
        "            'hidden_size': trial.suggest_categorical('hidden_size',[16, 32, 64, 128, 256, 512, 1024]),\n",
        "            'num_layers': trial.suggest_int('num_layers', 1, 5, 1),       \n",
        "        }\n",
        "\n",
        "    torch.manual_seed(42)\n",
        "\n",
        "    log_interval = 5\n",
        "    num_classes = 1 # parameter에서 빼서 상수로 설정\n",
        "    num_epochs = 10 # parameter에서 빼서 상수로 설정\n",
        "\n",
        "    train_dl, val_dl, input_size = get_data_loader(x, y,  cfg['batch_size'])\n",
        "    \n",
        "    model = LSTM(\n",
        "        num_classes = num_classes, \n",
        "        input_size = input_size, \n",
        "        hidden_size = cfg['hidden_size'], \n",
        "        num_layers = cfg['num_layers']\n",
        "    )\n",
        "    \n",
        "    if torch.cuda.is_available():\n",
        "        model = model.cuda()\n",
        "        \n",
        "    optimizer = optim.Adam(model.parameters(), lr=cfg['learning_rate'])\n",
        "    criterion = torch.nn.MSELoss()\n",
        "    best_loss = train(log_interval, model, train_dl, val_dl, optimizer, criterion, num_epochs)\n",
        "\n",
        "    print('best loss for the trial = ', best_loss)\n",
        "    predict_data = []\n",
        "    # 여기서 x는 (sample, lookback_length, 1)의 크기를 지님. 따라서, 제일 앞의 시점을 제거하려면, x[:, -1, :]이 되어야 함\n",
        "    x_pred = np.expand_dims(x_for_metric, 0)  # Inference에 사용할 lookback data를 x_pred로 지정. 앞으로 x_pred를 하나씩 옮겨 가면서 inference를 할 예정\n",
        "\n",
        "    for j, i in enumerate(range(max_prediction_length)):\n",
        "\n",
        "        # feed the last forecast back to the model as an input\n",
        "        x_pred = np.append( x_pred[:, 1:, :], np.expand_dims(y_for_metric[j, :], (0,2)), axis=1)\n",
        "        xt_pred = torch.Tensor(x_pred)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            xt_pred = xt_pred.cuda()\n",
        "        # generate the next forecast\n",
        "        yt_pred = model(xt_pred)\n",
        "        # tensor to array\n",
        "        # x_pred = xt_pred.cpu().detach().numpy()\n",
        "        y_pred = yt_pred.cpu().detach().numpy()\n",
        "\n",
        "        # save the forecast\n",
        "        predict_data.append(y_pred)\n",
        "\n",
        "    # transform the forecasts back to the original scale\n",
        "    predict_data = np.array(predict_data).reshape(-1, 1)\n",
        "    SMAPE = smape(y_for_metric, predict_data)\n",
        "    \n",
        "    print(f' \\nSMAPE : {SMAPE}')\n",
        "\n",
        "\n",
        "    return SMAPE"
      ],
      "metadata": {
        "id": "286VOvX3cdW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampler = optuna.samplers.TPESampler()\n",
        "#   sampler = optuna.samplers.SkoptSampler()\n",
        "\n",
        "# model.load_state_dict(torch.load('lstm_saved_model.pth'))\n",
        "    \n",
        "study = optuna.create_study(sampler=sampler, direction='minimize')\n",
        "study.optimize(objective, n_trials= 5)"
      ],
      "metadata": {
        "id": "JkM9FdqMcm7g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}